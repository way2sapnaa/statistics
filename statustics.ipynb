{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUyIlOY5TdCT"
      },
      "outputs": [],
      "source": [
        "\n",
        "1] What is statistics, and why is it important\n",
        "\n",
        "Statistics is the study of collecting, analyzing, interpreting, presenting, and organizing data. It is important because -\n",
        "\n",
        "Makes sense of data — turns raw data into insights\n",
        "\n",
        "Supports decision-making — in business, science, etc.\n",
        "\n",
        "Identifies patterns/trends — helps in forecasting\n",
        "\n",
        "Validates assumptions — through hypothesis testing\n",
        "\n",
        "Used in data science, ML, research, quality control, etc.\n",
        "\n",
        "2] What are the two main types of statistics\n",
        "\n",
        "Two main types of statitics are-\n",
        "Descriptive statistics\n",
        "Inferential statistics\n",
        "3] What are descriptive statistics\n",
        "\n",
        "Descriptive statistics are a branch of statistics that focuses on summarizing, organizing, and presenting the main features and characteristics of a dataset.\n",
        "4] What is inferential statistics\n",
        "\n",
        "Inferential statistics is a powerful branch of statistics that allows us to make generalizations and draw conclusions about a larger population based on data collected from a sample of that population.\n",
        "5] What is sampling in statistics\n",
        "\n",
        "In statistics, sampling is the process of selecting a subset of individuals or data points from a larger group (called the population) to gather information and make inferences about the entire population\n",
        "6] What are the different types of sampling methods\n",
        "\n",
        "Random Sampling\n",
        "Systematic Sampling\n",
        "Stratified Sampling\n",
        "Cluster Sampling\n",
        "Convenience Sampling\n",
        "Quota Sampling\n",
        "7] What is the difference between random and non-random sampling\n",
        "\n",
        "Random Sampling:\n",
        "\n",
        "Each member has equal chance of being selected\n",
        "\n",
        "Unbiased, more accurate\n",
        "\n",
        "Example: Lottery draw\n",
        "\n",
        "8] Define and give examples of qualitative and quantitative data\n",
        "\n",
        "Qualitative Data (Categorical): They describes qualities or categories and are Non-numeric\n",
        "\n",
        "Examples:\n",
        "\n",
        "Colors (Red, Blue)\n",
        "\n",
        "Gender (Male, Female)\n",
        "\n",
        "Feedback (Good, Bad)\n",
        "\n",
        "9] What are the different types of data in statistics\n",
        "\n",
        "2 Types of data in statistics:\n",
        "Qualitative Data\n",
        "Quantitative Data\n",
        "10] Explain nominal, ordinal, interval, and ratio levels of measurement\n",
        "\n",
        "Level of measurement:\n",
        "\n",
        "Nominal: Labels only, no order. Example: Gender, colors, city names\n",
        "Ordinal: Ordered, but gaps not meaningful. Example: Rank (1st, 2nd), rating (good, average, poor)\n",
        "Interval: Ordered, equal intervals, no true zero. Example: Temperature (°C), dates\n",
        "Ratio: Like interval, but has true zero. Example: Height, weight, income\n",
        "11] What is the measure of central tendency\n",
        "\n",
        "A measure of central tendency is a summary statistic that attempts to describe a whole set of data with a single value that represents the middle or center of its distribution. There are three main measures of central tendency: Mean, Median, Mode.\n",
        "12] Define mean, median, and mode\n",
        "\n",
        "Mean: Commonly referred to as the \"average.\" It is calculated by summing all the values in a dataset and then dividing by the total number of values.\n",
        "\n",
        "Median: The median is the middle value in a dataset when the values are arranged in ascending or descending order. It divides the data into two equal halves.\n",
        "\n",
        "Mode: The mode is the value that appears most frequently in a dataset.\n",
        "\n",
        "13] What is the significance of the measure of central tendency\n",
        "\n",
        "The measure of central tendency is significant because it provides a single value that represents the center or typical value of a dataset. It helps simplify complex data, making it easier to understand, compare, and analyze. It is widely used in decision-making, trend analysis, and interpreting statistical results.\n",
        "14] What is variance, and how is it calculated\n",
        "\n",
        "Variance measures how much the data values deviate from the mean. It shows the spread or dispersion of a dataset. To calculate it, subtract the mean from each data point, square the result, then take the average of those squared differences. For a population, divide by the number of data points (N); for a sample, divide by (N - 1).\n",
        "15] What is standard deviation, and why is it important\n",
        "\n",
        "Standard deviation is the square root of variance. It shows how much data values deviate, on average, from the mean. It is important because it gives a clear sense of data spread in the same units as the data, making it easier to interpret variability and consistency within a dataset.\n",
        "16] Define and explain the term range in statistics\n",
        "\n",
        "In statistics, the range is the difference between the highest and lowest values in a dataset. It shows how spread out the data is. For example, if the highest value is 90 and the lowest is 30, the range is 90 - 30 = 60. It's a simple measure of variability but doesn't reflect how data is distributed between the extremes.\n",
        "17] What is the difference between variance and standard deviation\n",
        "\n",
        "Variance measures the average of the squared differences from the mean, while standard deviation is the square root of variance. Both show data spread, but standard deviation is in the same unit as the data, making it easier to interpret and compare.\n",
        "18] What is skewness in a dataset\n",
        "\n",
        "Skewness measures the asymmetry of a dataset’s distribution.\n",
        "19] What does it mean if a dataset is positively or negatively skewed\n",
        "\n",
        "If the data is skewed to the right (positive skew), the tail is longer on the right side. If it's skewed to the left (negative skew), the tail is longer on the left.\n",
        "20] Define and explain kurtosis\n",
        "\n",
        "Kurtosis measures the \"tailedness\" or peakedness of a data distribution. It indicates how much of the data is in the tails and center compared to a normal distribution.\n",
        "21] What is the purpose of covariance\n",
        "\n",
        "The purpose of covariance is to measure how two variables change together. If both variables increase or decrease simultaneously, the covariance is positive. If one increases while the other decreases, it’s negative. It helps indicate the direction of the relationship between variables.\n",
        "22] What is the purpose of correlation\n",
        "\n",
        "Correlation measures the strength and direction of a linear relationship between two variables. It tells how closely the variables move together. The value ranges from −1 to +1, where +1 indicates a perfect positive relationship, −1 a perfect negative relationship, and 0 means no linear relationship.\n",
        "23] What is the difference between covariance and correlation\n",
        "\n",
        "Covariance measures the direction of the relationship between two variables, while correlation measures both the direction and strength of that relationship. Covariance is unscaled and depends on the units of the variables, making it harder to interpret. Correlation, on the other hand, is standardized and ranges from −1 to +1, making it easier to compare across different datasets.\n",
        "24] What are some real-world applications of statistics\n",
        "\n",
        "Statistics is widely used in real-world applications such as analyzing market trends in business, forecasting weather in meteorology, measuring effectiveness of drugs in healthcare, improving quality in manufacturing, conducting opinion polls in politics, and interpreting data in research and education. It helps in making informed decisions based on data.\n",
        "\n",
        "# PRACTICAL QUESTIONS\n",
        "# 1] How do you calculate the mean, median, and mode of a dataset\n",
        "\n",
        "import statistics\n",
        "\n",
        "data = [2, 4, 4, 6, 8, 10, 10, 10, 12]\n",
        "\n",
        "mean = statistics.mean(data)\n",
        "\n",
        "median = statistics.median(data)\n",
        "\n",
        "mode = statistics.mode(data)\n",
        "\n",
        "print(\"Mean:\", mean)\n",
        "print(\"Median:\", median)\n",
        "print(\"Mode:\", mode)\n",
        "\n",
        "\n",
        "Mean: 7.333333333333333\n",
        "Median: 8\n",
        "Mode: 10\n",
        "\n",
        "# 2] Write a Python program to compute the variance and standard deviation of a dataset\n",
        "import statistics\n",
        "\n",
        "data = [10, 12, 23, 23, 16, 23, 21, 16]\n",
        "\n",
        "variance = statistics.variance(data)\n",
        "\n",
        "std_dev = statistics.stdev(data)\n",
        "\n",
        "print(\"Variance:\", variance)\n",
        "print(\"Standard Deviation:\", std_dev)\n",
        "\n",
        "\n",
        "Variance: 27.428571428571427\n",
        "Standard Deviation: 5.237229365663818\n",
        "\n",
        "# 3] Create a dataset and classify it into nominal, ordinal, interval, and ratio types\n",
        "dataset = {\n",
        "    \"Name\": [\"Alice\", \"Bob\", \"Charlie\", \"David\"],             # Nominal\n",
        "    \"Satisfaction Level\": [\"Low\", \"Medium\", \"High\", \"High\"],  # Ordinal\n",
        "    \"Temperature (°C)\": [22, 25, 18, 30],                      # Interval\n",
        "    \"Income ($)\": [35000, 42000, 39000, 50000]                # Ratio\n",
        "}\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(dataset)\n",
        "print(df)\n",
        "\n",
        "\n",
        "      Name Satisfaction Level  Temperature (°C)  Income ($)\n",
        "0    Alice                Low                22       35000\n",
        "1      Bob             Medium                25       42000\n",
        "2  Charlie               High                18       39000\n",
        "3    David               High                30       50000\n",
        "\n",
        "# 4] Implement sampling techniques like random sampling and stratified sampling\n",
        "import pandas as pd\n",
        "\n",
        "# Sample dataset\n",
        "data = {'Name': ['A', 'B', 'C', 'D', 'E', 'F'],\n",
        "        'Age': [25, 30, 45, 22, 23, 36]}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Simple random sample of 3 rows\n",
        "random_sample = df.sample(n=3, random_state=1)\n",
        "print(\"Random Sample:\\n\", random_sample)\n",
        "\n",
        "\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "# Add a categorical column for stratification\n",
        "df['Group'] = ['X', 'Y', 'X', 'Y', 'X', 'Y']  # Strata\n",
        "\n",
        "split = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=1)\n",
        "\n",
        "for train_index, test_index in split.split(df, df['Group']):\n",
        "    stratified_sample = df.iloc[test_index]\n",
        "\n",
        "print(\"Stratified Sample:\\n\", stratified_sample)\n",
        "\n",
        "\n",
        "Random Sample:\n",
        "   Name  Age\n",
        "2    C   45\n",
        "1    B   30\n",
        "4    E   23\n",
        "Stratified Sample:\n",
        "   Name  Age Group\n",
        "5    F   36     Y\n",
        "3    D   22     Y\n",
        "0    A   25     X\n",
        "\n",
        "# 5] Write a Python function to calculate the range of a dataset\n",
        "def calculate_range(data):\n",
        "    return max(data) - min(data)\n",
        "\n",
        "data = [10, 5, 8, 22, 13]\n",
        "range_value = calculate_range(data)\n",
        "print(\"Range of the dataset:\", range_value)\n",
        "\n",
        "\n",
        "Range of the dataset: 17\n",
        "\n",
        "# 6] Create a dataset and plot its histogram to visualize skewness\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "data = np.random.exponential(scale=2.0, size=1000)\n",
        "\n",
        "# Plot histogram\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.histplot(data, bins=30, kde=True, color='magenta')\n",
        "plt.title('Histogram to Visualize Skewness')\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " # 7] Calculate skewness and kurtosis of a dataset using Python libraries\n",
        " import numpy as np\n",
        "from scipy.stats import skew, kurtosis\n",
        "\n",
        "data = [12, 15, 12, 18, 19, 21, 24, 30, 45, 60]\n",
        "\n",
        "skewness = skew(data)\n",
        "\n",
        "kurt = kurtosis(data)  # By default, it's excess kurtosis (normal = 0)\n",
        "\n",
        "print(\"Skewness:\", skewness)\n",
        "print(\"Kurtosis:\", kurt)\n",
        "\n",
        "Skewness: 1.2695758039511995\n",
        "Kurtosis: 0.4079040856499203\n",
        "\n",
        "# 8] Generate a dataset and demonstrate positive and negative skewness\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import skew\n",
        "\n",
        "# Generate positively skewed data (right skew)\n",
        "positive_skew = np.random.exponential(scale=2.0, size=1000)\n",
        "\n",
        "# Generate negatively skewed data (left skew)\n",
        "negative_skew = -np.random.exponential(scale=2.0, size=1000) + 10\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Positive skew\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(positive_skew, bins=30, kde=True, color='skyblue')\n",
        "plt.title(f'Positive Skew (Skewness = {skew(positive_skew):.2f})')\n",
        "\n",
        "# Negative skew\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.histplot(negative_skew, bins=30, kde=True, color='salmon')\n",
        "plt.title(f'Negative Skew (Skewness = {skew(negative_skew):.2f})')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 9] Write a Python script to calculate covariance between two datasets\n",
        "import numpy as np\n",
        "\n",
        "# Sample datasets\n",
        "x = [2, 4, 6, 8, 10]\n",
        "y = [1, 3, 2, 5, 7]\n",
        "\n",
        "# Covariance matrix\n",
        "cov_matrix = np.cov(x, y, bias=False)  # bias=False for sample covariance\n",
        "\n",
        "# Covariance value\n",
        "cov_xy = cov_matrix[0][1]\n",
        "\n",
        "print(\"Covariance (x, y):\", cov_xy)\n",
        "\n",
        "\n",
        "Covariance (x, y): 7.0\n",
        "\n",
        "# 10] Write a Python script to calculate the correlation coefficient between two datasets\n",
        "import numpy as np\n",
        "\n",
        "# Sample datasets\n",
        "x = [2, 4, 6, 8, 10]\n",
        "y = [1, 3, 2, 5, 7]\n",
        "\n",
        "# Correlation matrix\n",
        "corr_matrix = np.corrcoef(x, y)\n",
        "\n",
        "# Correlation coefficient\n",
        "corr_xy = corr_matrix[0, 1]\n",
        "\n",
        "print(\"Correlation Coefficient (x, y):\", corr_xy)\n",
        "\n",
        "\n",
        "Correlation Coefficient (x, y): 0.9191450300180578\n",
        "\n",
        "# 11] Create a scatter plot to visualize the relationship between two variables\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sample datasets\n",
        "x = [2, 4, 6, 8, 10]\n",
        "y = [1, 3, 2, 5, 7]\n",
        "\n",
        "# Scatter plot\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.scatter(x, y, color='red', marker='o')\n",
        "plt.title('Scatter Plot: X vs Y')\n",
        "plt.xlabel('X Values')\n",
        "plt.ylabel('Y Values')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 12] Implement and compare simple random sampling and systematic sampling\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Create dataset\n",
        "data = pd.DataFrame({'ID': np.arange(1, 101)})\n",
        "\n",
        "# Simple Random Sample of 10 rows\n",
        "simple_random_sample = data.sample(n=10, random_state=1)\n",
        "print(\"Simple Random Sample:\\n\", simple_random_sample)\n",
        "\n",
        "\n",
        "def systematic_sampling(df, step):\n",
        "    start = np.random.randint(0, step)\n",
        "    indexes = np.arange(start, len(df), step)\n",
        "    return df.iloc[indexes]\n",
        "\n",
        "# Systematic Sample (every 10th item)\n",
        "systematic_sample = systematic_sampling(data, step=10)\n",
        "print(\"\\nSystematic Sample:\\n\", systematic_sample)\n",
        "\n",
        "\n",
        "Simple Random Sample:\n",
        "     ID\n",
        "80  81\n",
        "84  85\n",
        "33  34\n",
        "81  82\n",
        "93  94\n",
        "17  18\n",
        "36  37\n",
        "82  83\n",
        "69  70\n",
        "65  66\n",
        "\n",
        "Systematic Sample:\n",
        "     ID\n",
        "0    1\n",
        "10  11\n",
        "20  21\n",
        "30  31\n",
        "40  41\n",
        "50  51\n",
        "60  61\n",
        "70  71\n",
        "80  81\n",
        "90  91\n",
        "\n",
        "# 13] Calculate the mean, median, and mode of grouped data\n",
        "# Grouped data\n",
        "classes = [(0, 10), (10, 20), (20, 30), (30, 40)]\n",
        "frequencies = [5, 8, 15, 7]\n",
        "\n",
        "# Midpoints\n",
        "midpoints = [(a + b) / 2 for a, b in classes]\n",
        "\n",
        "# Mean\n",
        "total_freq = sum(frequencies)\n",
        "mean = sum(f * m for f, m in zip(frequencies, midpoints)) / total_freq\n",
        "print(\"Mean:\", mean)\n",
        "\n",
        "# Median\n",
        "cumulative = [sum(frequencies[:i+1]) for i in range(len(frequencies))]\n",
        "n = total_freq\n",
        "for i, cf in enumerate(cumulative):\n",
        "    if cf >= n / 2:\n",
        "        median_class = i\n",
        "        break\n",
        "\n",
        "L = classes[median_class][0]\n",
        "F = cumulative[median_class - 1] if median_class > 0 else 0\n",
        "f = frequencies[median_class]\n",
        "h = classes[median_class][1] - classes[median_class][0]\n",
        "median = L + ((n/2 - F) / f) * h\n",
        "print(\"Median:\", median)\n",
        "\n",
        "# Mode\n",
        "modal = frequencies.index(max(frequencies))\n",
        "L = classes[modal][0]\n",
        "f1 = frequencies[modal]\n",
        "f0 = frequencies[modal - 1] if modal > 0 else 0\n",
        "f2 = frequencies[modal + 1] if modal < len(frequencies) - 1 else 0\n",
        "mode = L + ((f1 - f0) / ((2*f1) - f0 - f2)) * h\n",
        "print(\"Mode:\", mode)\n",
        "\n",
        "\n",
        "Mean: 21.857142857142858\n",
        "Median: 23.0\n",
        "Mode: 24.666666666666668\n",
        "\n",
        "# 14] Simulate data using Python and calculate its central tendency and dispersion\n",
        "import numpy as np\n",
        "import statistics\n",
        "\n",
        "# Simulate data: 100 values from normal distribution (mean=50, std=10)\n",
        "data = np.random.normal(loc=50, scale=10, size=100)\n",
        "\n",
        "# Central Tendency\n",
        "mean = np.mean(data)\n",
        "median = np.median(data)\n",
        "mode = statistics.mode(data.round())  # rounding for mode to work reliably\n",
        "\n",
        "# Dispersion\n",
        "data_range = np.max(data) - np.min(data)\n",
        "variance = np.var(data, ddof=1)  # sample variance\n",
        "std_dev = np.std(data, ddof=1)   # sample standard deviation\n",
        "\n",
        "# Results\n",
        "print(f\"Mean: {mean:.2f}\")\n",
        "print(f\"Median: {median:.2f}\")\n",
        "print(f\"Mode: {mode}\")\n",
        "print(f\"Range: {data_range:.2f}\")\n",
        "print(f\"Variance: {variance:.2f}\")\n",
        "print(f\"Standard Deviation: {std_dev:.2f}\")\n",
        "\n",
        "\n",
        "Mean: 49.56\n",
        "Median: 47.53\n",
        "Mode: 47.0\n",
        "Range: 55.43\n",
        "Variance: 120.70\n",
        "Standard Deviation: 10.99\n",
        "\n",
        "# 15] Use NumPy or pandas to summarize a dataset’s descriptive statistics\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Simulated dataset\n",
        "data = {\n",
        "    'Age': np.random.randint(18, 60, size=100),\n",
        "    'Salary': np.random.normal(loc=50000, scale=10000, size=100)\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Descriptive statistics summary\n",
        "print(df.describe())\n",
        "\n",
        "\n",
        "              Age        Salary\n",
        "count  100.000000    100.000000\n",
        "mean    38.500000  51217.038386\n",
        "std     12.175899   9260.882594\n",
        "min     18.000000  27345.303630\n",
        "25%     27.000000  45117.174372\n",
        "50%     38.500000  50618.486244\n",
        "75%     48.250000  58014.271364\n",
        "max     59.000000  76169.281518\n",
        "\n",
        "# 16] Plot a boxplot to understand the spread and identify outliers\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Simulate data\n",
        "data = np.random.normal(loc=50, scale=10, size=100)\n",
        "data = np.append(data, [100, 105])  # add outliers\n",
        "\n",
        "# Boxplot\n",
        "sns.boxplot(data=data, color='orange')\n",
        "plt.title(\"Boxplot to Identify Outliers\")\n",
        "plt.xlabel(\"Values\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 17] Calculate the interquartile range (IQR) of a dataset\n",
        "import numpy as np\n",
        "\n",
        "# Example dataset\n",
        "data = [12, 15, 14, 10, 18, 21, 22, 25, 30, 35]\n",
        "\n",
        "# Calculate Q1 and Q3\n",
        "Q1 = np.percentile(data, 25)\n",
        "Q3 = np.percentile(data, 75)\n",
        "\n",
        "# IQR\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "print(\"Q1 (25th percentile):\", Q1)\n",
        "print(\"Q3 (75th percentile):\", Q3)\n",
        "print(\"Interquartile Range (IQR):\", IQR)\n",
        "\n",
        "\n",
        "\n",
        "Q1 (25th percentile): 14.25\n",
        "Q3 (75th percentile): 24.25\n",
        "Interquartile Range (IQR): 10.0\n",
        "\n",
        "# 18] Implement Z-score normalization and explain its significance\n",
        "import numpy as np\n",
        "\n",
        "# Example dataset\n",
        "data = [10, 12, 14, 18, 20, 22, 24]\n",
        "\n",
        "# Convert to NumPy array\n",
        "data = np.array(data)\n",
        "\n",
        "# Calculate mean and std\n",
        "mean = np.mean(data)\n",
        "std = np.std(data)\n",
        "\n",
        "# Z-score normalization\n",
        "z_scores = (data - mean) / std\n",
        "\n",
        "print(\"Original Data:\", data)\n",
        "print(\"Z-scores:\", z_scores)\n",
        "\n",
        "\n",
        "Original Data: [10 12 14 18 20 22 24]\n",
        "Z-scores: [-1.46301434 -1.05337032 -0.64372631  0.17556172  0.58520574  0.99484975\n",
        "  1.40449377]\n",
        "\n",
        "# 19] Compare two datasets using their standard deviations\n",
        "import numpy as np\n",
        "\n",
        "# Dataset A (more consistent)\n",
        "data_A = [50, 52, 51, 49, 50, 51, 52]\n",
        "\n",
        "# Dataset B (more variable)\n",
        "data_B = [40, 60, 45, 70, 30, 80, 55]\n",
        "\n",
        "# Standard deviations\n",
        "std_A = np.std(data_A, ddof=1)\n",
        "std_B = np.std(data_B, ddof=1)\n",
        "\n",
        "print(\"Standard Deviation of A:\", round(std_A, 2))\n",
        "print(\"Standard Deviation of B:\", round(std_B, 2))\n",
        "\n",
        "# Interpretation\n",
        "if std_A < std_B:\n",
        "    print(\"Dataset A is more consistent (less spread).\")\n",
        "else:\n",
        "    print(\"Dataset B is more consistent (less spread).\")\n",
        "\n",
        "\n",
        "Standard Deviation of A: 1.11\n",
        "Standard Deviation of B: 17.42\n",
        "Dataset A is more consistent (less spread).\n",
        "\n",
        "# 20] Write a Python program to visualize covariance using a heatmap\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Simulate a dataset\n",
        "np.random.seed(0)\n",
        "data = {\n",
        "    'Math': np.random.normal(70, 10, 100),\n",
        "    'Science': np.random.normal(75, 12, 100),\n",
        "    'English': np.random.normal(65, 8, 100)\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Calculate covariance matrix\n",
        "cov_matrix = df.cov()\n",
        "\n",
        "# Plot heatmap\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cov_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title(\"Covariance Heatmap\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 21] Use seaborn to create a correlation matrix for a dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Simulate dataset\n",
        "np.random.seed(1)\n",
        "data = {\n",
        "    'Math': np.random.normal(70, 10, 100),\n",
        "    'Science': np.random.normal(75, 12, 100),\n",
        "    'English': np.random.normal(65, 8, 100),\n",
        "    'History': np.random.normal(60, 9, 100)\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Compute correlation matrix\n",
        "corr_matrix = df.corr()\n",
        "\n",
        "# Plot correlation heatmap\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title(\"Correlation Matrix Heatmap\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 22] Generate a dataset and implement both variance and standard deviation computations\n",
        "import numpy as np\n",
        "\n",
        "# Generate dataset: 100 values from normal distribution (mean=50, std=10)\n",
        "data = np.random.normal(loc=50, scale=10, size=100)\n",
        "\n",
        "# Variance (sample)\n",
        "variance = np.var(data, ddof=1)\n",
        "\n",
        "# Standard Deviation (sample)\n",
        "std_dev = np.std(data, ddof=1)\n",
        "\n",
        "# Output results\n",
        "print(\"Generated Data (first 10):\", data[:10])\n",
        "print(\"Variance:\", round(variance, 2))\n",
        "print(\"Standard Deviation:\", round(std_dev, 2))\n",
        "\n",
        "\n",
        "Generated Data (first 10): [36.93465927 50.7638048  53.67231814 62.32899192 45.77143039 50.86464407\n",
        " 28.57533271 41.69831136 54.51615951 61.04174326]\n",
        "Variance: 108.32\n",
        "Standard Deviation: 10.41\n",
        "\n",
        "# 23] Visualize skewness and kurtosis using Python libraries like matplotlib or seaborn\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import skew, kurtosis\n",
        "\n",
        "# Generate dataset (positively skewed)\n",
        "data = np.random.exponential(scale=2, size=1000)\n",
        "\n",
        "# Calculate skewness and kurtosis\n",
        "skewness = skew(data)\n",
        "kurt = kurtosis(data)  # excess kurtosis (normal dist = 0)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.histplot(data, bins=30, kde=True, color='grey')\n",
        "plt.title(f\"Histogram\\nSkewness = {skewness:.2f}, Kurtosis = {kurt:.2f}\")\n",
        "plt.xlabel(\"Value\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 24] Implement the Pearson and Spearman correlation coefficients for a dataset\n",
        "import numpy as np\n",
        "from scipy.stats import pearsonr, spearmanr\n",
        "\n",
        "# Sample dataset\n",
        "x = [10, 20, 30, 40, 50]\n",
        "y = [15, 25, 35, 45, 65]  # Linear, but one point shifted\n",
        "\n",
        "# Pearson Correlation (measures linear relationship)\n",
        "pearson_corr, _ = pearsonr(x, y)\n",
        "\n",
        "# Spearman Correlation (measures rank-based/monotonic relationship)\n",
        "spearman_corr, _ = spearmanr(x, y)\n",
        "\n",
        "print(\"Pearson Correlation Coefficient:\", round(pearson_corr, 3))\n",
        "print(\"Spearman Correlation Coefficient:\", round(spearman_corr, 3))\n",
        "\n",
        "\n",
        "Pearson Correlation Coefficient: 0.986\n",
        "Spearman Correlation Coefficient: 1.0"
      ]
    }
  ]
}